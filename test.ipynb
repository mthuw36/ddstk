{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b8bdfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Qchat-10-Score Sex       Ethnicity Jaundice Family_mem_with_ASD  \\\n",
      "0                  3   f  middle eastern      yes                  no   \n",
      "1                  4   m  White European      yes                  no   \n",
      "2                  4   m  middle eastern      yes                  no   \n",
      "3                 10   m        Hispanic       no                  no   \n",
      "4                  9   f  White European       no                 yes   \n",
      "...              ...  ..             ...      ...                 ...   \n",
      "1049               1   f  White European       no                 yes   \n",
      "1050               5   m           black      yes                  no   \n",
      "1051               9   m  middle eastern      yes                  no   \n",
      "1052               3   m  White European       no                 yes   \n",
      "1053               6   m           asian      yes                 yes   \n",
      "\n",
      "     Class/ASD Traits  \n",
      "0                  No  \n",
      "1                 Yes  \n",
      "2                 Yes  \n",
      "3                 Yes  \n",
      "4                 Yes  \n",
      "...               ...  \n",
      "1049               No  \n",
      "1050              Yes  \n",
      "1051              Yes  \n",
      "1052               No  \n",
      "1053              Yes  \n",
      "\n",
      "[1054 rows x 6 columns]\n",
      "      Jaundice  Sex  Qchat-10-Score__0  Qchat-10-Score__1  Qchat-10-Score__2  \\\n",
      "0            1    1                  0                  0                  0   \n",
      "1            1    0                  0                  0                  0   \n",
      "2            1    0                  0                  0                  0   \n",
      "3            0    0                  0                  0                  0   \n",
      "4            0    1                  0                  0                  0   \n",
      "...        ...  ...                ...                ...                ...   \n",
      "1049         0    1                  0                  1                  0   \n",
      "1050         1    0                  0                  0                  0   \n",
      "1051         1    0                  0                  0                  0   \n",
      "1052         0    0                  0                  0                  0   \n",
      "1053         1    0                  0                  0                  0   \n",
      "\n",
      "      Qchat-10-Score__3  Qchat-10-Score__4  Qchat-10-Score__5  \\\n",
      "0                     1                  0                  0   \n",
      "1                     0                  1                  0   \n",
      "2                     0                  1                  0   \n",
      "3                     0                  0                  0   \n",
      "4                     0                  0                  0   \n",
      "...                 ...                ...                ...   \n",
      "1049                  0                  0                  0   \n",
      "1050                  0                  0                  1   \n",
      "1051                  0                  0                  0   \n",
      "1052                  1                  0                  0   \n",
      "1053                  0                  0                  0   \n",
      "\n",
      "      Qchat-10-Score__6  Qchat-10-Score__7  ...  Native Indian  Others  \\\n",
      "0                     0                  0  ...              0       0   \n",
      "1                     0                  0  ...              0       0   \n",
      "2                     0                  0  ...              0       0   \n",
      "3                     0                  0  ...              0       0   \n",
      "4                     0                  0  ...              0       0   \n",
      "...                 ...                ...  ...            ...     ...   \n",
      "1049                  0                  0  ...              0       0   \n",
      "1050                  0                  0  ...              0       0   \n",
      "1051                  0                  0  ...              0       0   \n",
      "1052                  0                  0  ...              0       0   \n",
      "1053                  1                  0  ...              0       0   \n",
      "\n",
      "      Pacifica  White European  asian  black  middle eastern  mixed  \\\n",
      "0            0               0      0      0               1      0   \n",
      "1            0               1      0      0               0      0   \n",
      "2            0               0      0      0               1      0   \n",
      "3            0               0      0      0               0      0   \n",
      "4            0               1      0      0               0      0   \n",
      "...        ...             ...    ...    ...             ...    ...   \n",
      "1049         0               1      0      0               0      0   \n",
      "1050         0               0      0      1               0      0   \n",
      "1051         0               0      0      0               1      0   \n",
      "1052         0               1      0      0               0      0   \n",
      "1053         0               0      1      0               0      0   \n",
      "\n",
      "      south asian  Class/ASD Traits  \n",
      "0               0                 0  \n",
      "1               0                 1  \n",
      "2               0                 1  \n",
      "3               0                 1  \n",
      "4               0                 1  \n",
      "...           ...               ...  \n",
      "1049            0                 0  \n",
      "1050            0                 1  \n",
      "1051            0                 1  \n",
      "1052            0                 0  \n",
      "1053            0                 1  \n",
      "\n",
      "[1054 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.layers import Conv2D , MaxPooling2D\n",
    "from keras import backend as k\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Đọc dữ liệu từ tệp CSV\n",
    "data = pd.read_csv(r\"C:\\Users\\MinhThw\\OneDrive\\Desktop\\Autism data ADS\\Autism Dataset for Toddlers.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "# Đặt tên cột cho DataFrame\n",
    "#raw_data.columns = ['Qchat-10-Score', 'Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD', 'Who completed the test', 'Class/ASD Traits']\n",
    "raw_data=data.iloc[:, [12,13,14,15,16,18]]\n",
    "\n",
    "print (raw_data)\n",
    "\n",
    "# Biến đổi dữ liệu\n",
    "result = pd.concat([\n",
    "    pd.get_dummies(raw_data['Jaundice']).drop(['no'], axis=1).rename(columns={'yes': 'Jaundice'}),\n",
    "    pd.get_dummies(raw_data['Sex']).drop(['m'], axis=1).rename(columns={'f': 'Sex'}),\n",
    "    pd.get_dummies(raw_data['Qchat-10-Score'], prefix='Qchat-10-Score_'),\n",
    "    pd.get_dummies(raw_data['Family_mem_with_ASD']).drop(['no'], axis=1).rename(columns={'yes': 'Family_mem_with_ASD'}),\n",
    "    pd.get_dummies(raw_data['Ethnicity']),\n",
    "    #pd.get_dummies(raw_data['Who completed the test']).replace({'family member': 1, 'other': 0}).rename(columns={'family member': 'Who completed the test'}),\n",
    "    pd.get_dummies(raw_data['Class/ASD Traits']).drop(['No'], axis=1).rename(columns={'Yes': 'Class/ASD Traits'})\n",
    "], axis=1)\n",
    "\n",
    "# In kết quả\n",
    "print((result).astype(int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ddc09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jaundice', 'Sex', 'Qchat-10-Score__0', 'Qchat-10-Score__1',\n",
       "       'Qchat-10-Score__2', 'Qchat-10-Score__3', 'Qchat-10-Score__4',\n",
       "       'Qchat-10-Score__5', 'Qchat-10-Score__6', 'Qchat-10-Score__7',\n",
       "       'Qchat-10-Score__8', 'Qchat-10-Score__9', 'Qchat-10-Score__10',\n",
       "       'Family_mem_with_ASD', 'Hispanic', 'Latino', 'Native Indian',\n",
       "       'Others', 'Pacifica', 'White European', 'asian', 'black',\n",
       "       'middle eastern', 'mixed', 'south asian', 'Class/ASD Traits'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38661a01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1054, 25)\n",
      "(737, 25) (737,)\n",
      "(317, 25) (317,)\n",
      "(737, 25) (737,)\n",
      "(317, 25) (317,)\n",
      "737 Train Sample\n",
      "317 Test Sample\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 1s 47ms/step - loss: 3.9826 - accuracy: 0.3012 - val_loss: 0.9761 - val_accuracy: 0.3344\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8452 - accuracy: 0.3826 - val_loss: 0.6755 - val_accuracy: 0.5521\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7348 - accuracy: 0.6201 - val_loss: 0.7561 - val_accuracy: 0.6751\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7723 - accuracy: 0.7123 - val_loss: 0.7436 - val_accuracy: 0.7319\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7081 - accuracy: 0.7517 - val_loss: 0.5658 - val_accuracy: 0.7697\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5572 - accuracy: 0.7883 - val_loss: 0.4125 - val_accuracy: 0.7950\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3996 - accuracy: 0.8535 - val_loss: 0.3799 - val_accuracy: 0.8959\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3560 - accuracy: 0.9077 - val_loss: 0.3461 - val_accuracy: 0.9211\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3014 - accuracy: 0.9376 - val_loss: 0.2802 - val_accuracy: 0.9211\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.2379 - accuracy: 0.9349 - val_loss: 0.2344 - val_accuracy: 0.9180\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1908 - accuracy: 0.9430 - val_loss: 0.1806 - val_accuracy: 0.9464\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1360 - accuracy: 0.9729 - val_loss: 0.1232 - val_accuracy: 0.9779\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0863 - accuracy: 0.9946 - val_loss: 0.0787 - val_accuracy: 0.9874\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0459 - accuracy: 0.9959 - val_loss: 0.0386 - val_accuracy: 0.9905\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0195 - accuracy: 0.9959 - val_loss: 0.0184 - val_accuracy: 0.9968\n",
      "The Model has Successfully Trained\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#X = result.to_numpy()\n",
    "  \n",
    "\n",
    "# Chia thành tập huấn luyện và tập kiểm tra\n",
    "data_row_dim = 1\n",
    "data_col_dim = 26\n",
    "X = result.drop(['Class/ASD Traits'], axis = 1).to_numpy()\n",
    "print(X.shape)\n",
    "#X = X.values.reshape(-1, data_row_dim, data_col_dim)\n",
    "#print(X.shape)   \n",
    "y_temp = result['Class/ASD Traits']\n",
    "# Chuyển đổi kiểu dữ liệu của y thành số thực\n",
    "y = y_temp.dropna().astype('float32').to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0],data_row_dim,data_col_dim,1)\n",
    "#X_test = X_test.reshape(X_test.shape[0],data_row_dim,data_col_dim,1)\n",
    "input_shape = (data_row_dim,data_col_dim,1)\n",
    "\n",
    "print(X_train.shape , y_train.shape)\n",
    "print(X_test.shape , y_test.shape)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#print('X_train.shape', X_test.shape)\n",
    "print(X_train.shape[0],'Train Sample')\n",
    "print(X_test.shape[0],'Test Sample')\n",
    "\n",
    "#Tạo Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32,activation='relu'))\n",
    "model.add(Dense(units=64,activation='relu'))\n",
    "model.add(Dense(units=128,activation='relu'))\n",
    "model.add(Dense(units=64,activation='relu'))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile model\n",
    "model.compile( loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"]) # categorical_crossentropy'\n",
    "#model.compile(loss = 'mse',optimizer = 'adam')\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train,y_train , batch_size=128,epochs=15,verbose=1,validation_data=(X_test,y_test))\n",
    "\n",
    "#model.fit(X_train,y_train, batch_size=128, validation_data=(X_test,y_test), epochs=40, verbose=1)\n",
    "print(\"The Model has Successfully Trained\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58216058",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.history.history.keys()\n",
    "\n",
    "model_loss = pd.DataFrame(model.history.history) # Bai toan regression ko co metric \"accuracy\"\n",
    "model_loss['loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_loss['loss'], label= 'loss')\n",
    "plt.plot(model_loss['val_loss'], label= 'val_loss')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y_pred = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Model Saving\n",
    "model.save('Book1.model')\n",
    "print('Saving the model as Autism_Dataset_for_Toddlers.h5')\n",
    "\n",
    "# Save and load model  \n",
    "\n",
    "new_model=load_model('Book1.model')\n",
    "predictions = new_model.predict(X_test)\n",
    "print(predictions)\n",
    "y_pred = tf.argmax(predictions, axis=-1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37369df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#f1_score = f1_score(y_test,y_pred,average='macro') \n",
    "#calculate the accuracy and store it in a\n",
    "#accuracy_score = accuracy_score(y_test,y_pred)\n",
    "\n",
    "#print(f'f1_score: {f1_score}')\n",
    "#print(f'accuracy_score: {accuracy_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1d7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
